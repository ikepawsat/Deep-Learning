{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbmWgBGyu030"
      },
      "source": [
        "[CSCI 3370] Lab 4: Neural Machine Translation\n",
        "\n",
        "Part 1 Posted: Wednesday, Oct 9, 2024\n",
        "\n",
        "Part 1 Due: Friday, Oct 18, 2024\n",
        "\n",
        "Part 2 Due: TBD\n",
        "\n",
        "\n",
        "Submission: please rename the .ipynb file as <your_username>_lab4.ipynb before you submit it to canvas. Example: yuanyua_lab4_part1.ipynb and yuanyua_lab4_part2.ipynb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvrXFjpkjFea"
      },
      "source": [
        "Make sure you fill your name and NetID below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ssnsDXmjjFed"
      },
      "outputs": [],
      "source": [
        "NAME = \"Ike Pawsat\"\n",
        "NET_ID = \"pawsat\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZReRnSNjFef"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkiOj-45jFef"
      },
      "source": [
        "---\n",
        "# Assignment 4: RNN, LSTM, Attention, and Transformers\n",
        "\n",
        "In this assignment, you will implement neural machine translation (NMT) models using:\n",
        "\n",
        "1. RNNs\n",
        "2. LSTMs and LSTMs with attention\n",
        "3. Transformers.\n",
        "\n",
        "As in the previous assignments, you will see code blocks that look like this:\n",
        "```python\n",
        "###############################################################################\n",
        "# TODO: Create a variable x with value 3.7\n",
        "###############################################################################\n",
        "pass\n",
        "# END OF YOUR CODE\n",
        "```\n",
        "\n",
        "You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n",
        "```python\n",
        "###############################################################################\n",
        "# TODO: Create a variable x with value 3.7\n",
        "###############################################################################\n",
        "x = 3.7\n",
        "# END OF YOUR CODE\n",
        "```\n",
        "\n",
        "Also, please remember:\n",
        "- Do not write or modify any code outside of code blocks\n",
        "- Do not add or delete any cells from the notebook. You may add new cells to perform scatch work, but delete them before submitting.\n",
        "- Run all cells before submitting. You will only get credit for code that has been run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjCVMXF6jFeg"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First let's import some libraries that will be useful in this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8lgN9_n7jFeg"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "def seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25QiOm7WjFeg"
      },
      "source": [
        "Make sure you are using the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KFM-5s3xjFeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c0b5fc3-0f02-4554-82ac-176ef0e0ee92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good to go!\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('Good to go!')\n",
        "else:\n",
        "  print('Please set GPU via Edit -> Notebook Settings.')\n",
        "device = torch.device('cuda:0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdvcn1RojFeh"
      },
      "source": [
        "For this assignment, we will use an English-to-French dataset. As shown below, the dataset contains multiple lines each of which has an English sentence and its French translation separated by a tab. In this problem, since English is translated to French, English is the source language and French is the target language. Note that each text sequence is of variable lengnth and can be just one sentence or a paragraph of multiple sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AhjVxfGujFeh"
      },
      "outputs": [],
      "source": [
        "def download_if_not_exist(file_name):\n",
        "\n",
        "  if not os.path.exists(file_name):\n",
        "    import urllib.request\n",
        "    DATA_URL = 'https://download.pytorch.org/tutorial/data.zip'\n",
        "\n",
        "    file_name, _ = urllib.request.urlretrieve(DATA_URL, './data.zip')\n",
        "\n",
        "  return file_name\n",
        "\n",
        "def read_raw(file_name):\n",
        "  file_name = download_if_not_exist(file_name)\n",
        "\n",
        "  with zipfile.ZipFile(file_name, 'r') as fzip:\n",
        "    raw_text = fzip.read(file_name.split('.')[-2][1:] + '/eng-fra.txt').decode('utf-8')\n",
        "  return raw_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QABmwfu2jFei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d533e0-65b7-4e40-9240-b6410aeee35d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVa !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Wow!\tÇa alors !\n",
            "Fire!\tAu feu !\n",
            "Help!\tÀ l'aide !\n",
            "Jump.\tSaute.\n",
            "Stop!\tÇa suffit !\n",
            "Stop!\tStop !\n",
            "Stop!\tArrête-toi !\n",
            "Wait!\tAttends !\n",
            "Wait!\tAttendez !\n",
            "I see.\tJe comprends.\n"
          ]
        }
      ],
      "source": [
        "raw_text = read_raw('./data.zip')\n",
        "print(raw_text[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23hR_uCdjFei"
      },
      "source": [
        "Next we'll do some preprocessing on this raw text. We need to replace special symbols (non-breaking spaces) with spaces, convert all characters to lower case, and insert a space between words and punctuation marks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S8_kduOojFei"
      },
      "outputs": [],
      "source": [
        "def preprocess_raw(text):\n",
        "  text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
        "  out = ''\n",
        "  for i, char in enumerate(text.lower()):\n",
        "    if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':\n",
        "      out += ' '\n",
        "    out += char\n",
        "  return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx4AxHEBjFei"
      },
      "source": [
        "We further split the source-target pairs into a source list and a target list. We use word-level tokenization here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iMp7CGZzjFej"
      },
      "outputs": [],
      "source": [
        "def split_source_target(text, max_len):\n",
        "  source, target = [], []\n",
        "  for i, line in enumerate(text.split('\\n')):\n",
        "    if i > 5000: # we only use 5000 pairs of translation\n",
        "      break\n",
        "    parts = line.split('\\t')\n",
        "    if len(parts) == 2:\n",
        "      src_tokens = parts[0].split(' ')\n",
        "      tgt_tokens = parts[1].split(' ')\n",
        "      if (len(src_tokens) <= max_len) and (len(tgt_tokens) <= max_len):\n",
        "        source.append(src_tokens)\n",
        "        target.append(tgt_tokens)\n",
        "  return source, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X1dDbXd0jFej"
      },
      "outputs": [],
      "source": [
        "def prepare_data(raw_text, max_len=10000):\n",
        "  text = preprocess_raw(raw_text)\n",
        "  source, target = split_source_target(text, max_len)\n",
        "  return source, target\n",
        "\n",
        "source, target = prepare_data(raw_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKHCvQ66jFej"
      },
      "source": [
        "Using the whole dataset takes too much memory, and it is hard to train with a large vocabulary. Thus, we will filter out some words by looking at the statistical properties of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "x-tkdp5tjFej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "821d1e86-19f7-414c-9479-ca971e21287e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def len_dis(text):\n",
        "  lens = [len(line) for line in text]\n",
        "  len_counter = collections.Counter(lens)\n",
        "\n",
        "  lens = np.array(list(len_counter.keys()))\n",
        "  sort_idx = np.argsort(lens)\n",
        "  lens_sort = lens[sort_idx]\n",
        "  len_counts = np.array(list(len_counter.values()))\n",
        "  len_counts_sort = len_counts[sort_idx]\n",
        "  p = np.cumsum(len_counts_sort) / len_counts_sort.sum()\n",
        "  return p, lens_sort\n",
        "\n",
        "src_p, src_lens_sort = len_dis(source)\n",
        "tgt_p, tgt_lens_sort = len_dis(target)\n",
        "plt.plot(src_lens_sort, src_p, 'r-', label='eng')\n",
        "plt.plot(tgt_lens_sort, tgt_p, 'g-', label='fra')\n",
        "plt.title('Cumulative Distribution of Sentence Length')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbf4sd8IjFej"
      },
      "source": [
        "From the above plots, we can see that more than 90% of the sentences have a length of less than 8. Thus, we can filter out sentences of length greater than 8. We also filter out words that occur less than 5 times in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a81p91LPjFej"
      },
      "outputs": [],
      "source": [
        "# hyper-param\n",
        "MAX_LEN = 8\n",
        "MIN_FREQ = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDigebR2jFek"
      },
      "source": [
        "### Build Vocabulary\n",
        "\n",
        "Each word needs a unique index, and the words that have been filtered out need a special token to represent them. The following class Vocab is used to build the vocabulary. Some basic helper functions or dictionaries are also provided:\n",
        "- Dictionary word2index: Convert word string into index:\n",
        "- Dictionary index2word: Convert index into word string\n",
        "- helper function _build_vocab(): Build dictionaries for converting from words to indices and vice versa\n",
        "- Word Counter, num_word: Record the total number of unique tokens in the vocabulary\n",
        "    \n",
        "There are 4 special tokens added in the vocabulary:\n",
        "- 'pad': padding token. Sentences shorter than MAX_LEN is padded by this symbol to make the length to MAX_LEN\n",
        "- 'bos': beginning of sentence. This indicates the beginning of a sentence\n",
        "- 'eos': end of sentence. This indicates the end of a sentence\n",
        "- 'unk': unknown word. This represents words that have been filtered out (words that are not in the vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "094DHXtjjFek"
      },
      "outputs": [],
      "source": [
        "class Vocab():\n",
        "  def __init__(self, name, tokens, min_freq):\n",
        "    self.name = name\n",
        "    self.index2word = {\n",
        "      0: 'pad',\n",
        "      1: 'bos',\n",
        "      2: 'eos',\n",
        "      3: 'unk'\n",
        "    }\n",
        "    self.word2index = {v: k for k, v in self.index2word.items()}\n",
        "    self.num_word = 4\n",
        "    token_freq = collections.Counter(tokens)\n",
        "    tokens = [token for token in tokens if token_freq[token] >= MIN_FREQ]\n",
        "    self._build_vocab(tokens)\n",
        "\n",
        "  def _build_vocab(self, tokens):\n",
        "    for token in tokens:\n",
        "      if token not in self.word2index:\n",
        "        self.word2index[token] = self.num_word\n",
        "        self.index2word[self.num_word] = token\n",
        "        self.num_word += 1\n",
        "\n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.word2index.get(tokens, self.word2index['unk'])\n",
        "    else:\n",
        "      return [self.__getitem__(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tcHfSy4jFek"
      },
      "source": [
        "### Build Dataset\n",
        "\n",
        "The dataset pipeline involves the following steps:\n",
        "- For target language, every sentence will be 'sandwiched' with the 'bos' token and the 'eos' token.\n",
        "- Every sentence that has a length less than MAX_LEN will be padded to the MAX_LEN with the *padding_token*.\n",
        "- The dataset should return the converted tensor and the corresponding valid length before padding.\n",
        "- We use the Pytorch *DataLoader* API to build the dataset generator.\n",
        "\n",
        "For the purposes of this assignment, we will train and evaluate on only the training data. This isn't ideal because we do not know if we are  overfitting to the training data, but it is fine for instructional purposes. In practice (eg. for your projects), you should make sure to split your data into training/validation/test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "l28_hezgjFek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4f07aa-fe9a-4641-c676-45b749e5996d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "Vocabulary size of source language: 433\n",
            "Vocabulary size of target language: 420\n",
            "Total number of sentence pairs: 4990\n"
          ]
        }
      ],
      "source": [
        "def build_vocab(name, tokens, min_freq):\n",
        "  tokens = [token for line in tokens for token in line]\n",
        "  return Vocab(name, tokens, min_freq)\n",
        "\n",
        "def build_vocabs(lang_src, lang_tgt, src_text, tgt_text):\n",
        "  vocab_src = build_vocab(lang_src, src_text, MIN_FREQ)\n",
        "  vocab_tgt = build_vocab(lang_tgt, tgt_text, MIN_FREQ)\n",
        "  return vocab_src, vocab_tgt\n",
        "\n",
        "def pad(line, padding_token):\n",
        "  return line + [padding_token] * (MAX_LEN + 2 - len(line))\n",
        "\n",
        "def build_tensor(text, lang, is_source):\n",
        "  lines = [lang[line] for line in text]\n",
        "  if not is_source:\n",
        "    lines = [[lang['bos']] + line + [lang['eos']] for line in lines]\n",
        "  array = torch.tensor([pad(line, lang['pad']) for line in lines])\n",
        "  valid_len = (array != lang['pad']).sum(1)\n",
        "  return array, valid_len\n",
        "\n",
        "def load_data_nmt(batch_size=2):\n",
        "  lang_eng, lang_fra = build_vocabs('eng', 'fra', source, target)\n",
        "  src_array, src_valid_len = build_tensor(source, lang_eng, True)\n",
        "  tgt_array, tgt_valid_len = build_tensor(target, lang_fra, False)\n",
        "  train_data = torch.utils.data.TensorDataset(\n",
        "    src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
        "  print(train_data[0])\n",
        "  train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
        "  return lang_eng, lang_fra, train_iter\n",
        "\n",
        "\n",
        "source, target = prepare_data(raw_text, max_len=MAX_LEN)\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size=2)\n",
        "print('Vocabulary size of source language: {}'.format(vocab_eng.num_word))\n",
        "print('Vocabulary size of target language: {}'.format(vocab_fra.num_word))\n",
        "print('Total number of sentence pairs: {}'.format(len(source)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CzM0GP_jFek"
      },
      "source": [
        "## Part 1a: Sequence to Sequence with RNN (baseline)\n",
        "\n",
        "In this section, we provide the implementation of the seq2seq RNN baseline model. You do not need to implement any code in this section, but you should read and understand what the code is doing because you will need to implement something similar in subsequent sections. The following figure highlights the architecture of the seq2seq model. An encoder RNN encodes the input sequence into its hidden state, and passes the last hidden state to the decoder RNN. The decoder generates the target sequence.\n",
        "\n",
        "Implementation Details:\n",
        "\n",
        "- Embedding: We have represented each word with an integer or one-hot vector. We need an embedding layer to map an input word to its embedding vector.\n",
        "- Encoder: A vanilla RNN is used to encode a source sequence. The final hidden state is returned as output and passed to the decoder RNN.\n",
        "- Decoder: Another vanilla RNN is implemented to generate the target sequence. The hidden state is initialized with the last hidden state from the encoder.\n",
        "- Encoder-Decoder: The class NMTRNN is built by combining the encoder and the decoder, and yields the loss and predictions.\n",
        "- Loss: We have padded all sentences so that they have the same MAX_LEN. Thus, when we compute the loss, the loss from those padding_tokens should be masked out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJEe8Kb_jFek"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://raw.githubusercontent.com/dsgiitr/d2l-pytorch/24e89824c154c2afc419c5dadec9622e490b99bb/img/seq2seq.svg\" width=\"600\"/>\n",
        "</div>\n",
        "Image source: https://github.com/dsgiitr/d2l-pytorch/blob/master/img/seq2seq.svg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "evSc4MxwjFek"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim) # embedding layer\n",
        "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, sources, valid_len):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      source: tensor of size (N, T), where N is the batch size, T is the length of the sequence(s)\n",
        "      valid_len: tensor of size (N,), indicating the valid length of sequence(s) (the length before padding)\n",
        "    \"\"\"\n",
        "    word_embedded = self.embedding(sources)\n",
        "    N = word_embedded.shape[0]\n",
        "\n",
        "    h = sources.new_zeros(1, N, self.hidden_size).float() # initialize hidden state with zeros\n",
        "\n",
        "    o, h = self.enc(word_embedded, h)\n",
        "\n",
        "    return o[np.arange(N), valid_len] # return the hidden state of the valid last time step\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, h, target):\n",
        "    word_embedded = self.embedding(target)\n",
        "    N, T = word_embedded.shape[:2]\n",
        "\n",
        "    o, h = self.enc(word_embedded, h.view(1,N,self.hidden_size))\n",
        "    pred = self.output_emb(o)\n",
        "    return pred, h\n",
        "\n",
        "class NMTRNN(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size):\n",
        "    super(NMTRNN, self).__init__()\n",
        "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size)\n",
        "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size)\n",
        "\n",
        "  def forward(self, src, src_len, tgt, tgt_len):\n",
        "    h = self.enc(src, src_len)\n",
        "    T = tgt.shape[1]\n",
        "\n",
        "    pred, _ = self.dec(h, tgt)\n",
        "    loss = 0\n",
        "    for t in range(T-1):\n",
        "      # target sequence should shift by one time-step, because we are predicting the next word\n",
        "      # notice the `ignore_index` parameter is set to 0, which is for the `pad` token\n",
        "      loss = loss + F.nll_loss(F.log_softmax(pred[:, t]), tgt[:, t+1], ignore_index=0)\n",
        "\n",
        "    return loss, pred.argmax(dim=-1)\n",
        "\n",
        "  def predict(self, src, src_len, tgt, tgt_len):\n",
        "      \"\"\"\n",
        "      When predicting a sequence given the 'bos' token, the input for the next step is the predicted\n",
        "      token from the previous time step.\n",
        "      \"\"\"\n",
        "      h = self.enc(src, src_len)\n",
        "\n",
        "      inputs = tgt[:, :1]\n",
        "      preds = []\n",
        "      for t in range(MAX_LEN+1): # plus the 'eos' token\n",
        "        pred, h = self.dec(h, inputs)\n",
        "        preds.append(pred)\n",
        "        inputs = pred.argmax(dim=-1)\n",
        "\n",
        "      pred = torch.cat(preds, dim=1).argmax(dim=-1)\n",
        "      return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WoC98IWPjFel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25dd2b83-4dfd-4e23-f4d9-1b88d421679e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-a6e449a9f48b>:66: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  loss = loss + F.nll_loss(F.log_softmax(pred[:, t]), tgt[:, t+1], ignore_index=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([146,  65, 349,  53,   3, 330,  52, 330, 330, 330])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 156 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([14,  3,  3, 11,  2, 11,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([15, 89,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 312 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 14, 164,   3,  11,  11,   2,  11,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 48,  49, 113, 282,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 468 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 3,  3, 11,  3, 11,  2, 11,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([36,  3, 37,  3,  5,  2,  0,  0,  0])\n",
            "\n",
            "iter 624 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([36,  3,  3, 11,  2, 11,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([36,  9, 88, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 780 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([14, 28,  3, 11,  2, 11,  3, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 936 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 14,  28,   3, 227,  37,   3,  11,   2,  11,   3])\n",
            "\n",
            "tgt:\t tensor([ 14,  28,  35, 227,  37,   3,  11,   2,   0])\n",
            "\n",
            "iter 1092 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([171, 342,   3,  11,   2,  11,   3,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1248 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 14, 116,  72,  11,   2,  11,   3,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 14, 197,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1404 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 14, 171,   3,  11,   2,  11,   3,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([14, 70,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 1560 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([168, 203,  78,   3,  41,  24,   2,   5,   3,   3])\n",
            "\n",
            "tgt:\t tensor([168,  90,  79,   4,  41,  24,   2,   0,   0])\n",
            "\n",
            "iter 1716 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 14, 171,   3, 205,   5,   2,  11,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 14,  70,  17, 205,   5,   2,   0,   0,   0])\n",
            "\n",
            "iter 1872 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 3,  5,  2, 11,  3, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 2028 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 14, 171,   3,  75,   3,  11,   2,  11,   3,  11])\n",
            "\n",
            "tgt:\t tensor([ 14, 171, 213,  75,   3,  11,   2,   0,   0])\n",
            "\n",
            "iter 2184 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([176, 203,  14, 171,   3,  24,   2,  11,   3,   3])\n",
            "\n",
            "tgt:\t tensor([176, 203,  14, 116,   3,  24,   2,   0,   0])\n",
            "\n",
            "iter 2340 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([14, 28, 11,  2, 11,  3, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([103, 385,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 2496 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([15,  3, 72,  3, 11,  2, 11,  3, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 15, 291,   9,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2652 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 14, 116, 179, 259,  11,   2,  11,   3,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 14, 116, 179, 305,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2808 / 7800\tLoss:\t3.997143\n",
            "pred:\t tensor([ 36, 316,  41,  75,  11,   2,  11,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([315, 316,  41, 139,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2964 / 7800\tLoss:\t3.544607\n",
            "pred:\t tensor([ 3, 11,  2, 11,  3,  3,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 3120 / 7800\tLoss:\t3.656564\n",
            "pred:\t tensor([ 15, 282,  11,   2,  11,   3,   3,   3,  11,   3])\n",
            "\n",
            "tgt:\t tensor([  3, 265,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 3276 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([14, 28,  3, 11,  2, 11,  3, 11,  3, 11])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 3432 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 3, 72,  3, 11,  2, 11,  3, 11,  3, 11])\n",
            "\n",
            "tgt:\t tensor([  3,  72, 248,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3588 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([176,   3, 302,  24,   2,  24,   3,   5,   5,   5])\n",
            "\n",
            "tgt:\t tensor([  3,  37, 355,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3744 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 14, 116,  74,   3,  11,   2,  11,   3,  11,   3])\n",
            "\n",
            "tgt:\t tensor([ 14, 289,  74,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 3900 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([176,  72, 171,  24,   2,  24,   3,   5,   3,  11])\n",
            "\n",
            "tgt:\t tensor([176,   9, 237,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4056 / 7800\tLoss:\t3.954447\n",
            "pred:\t tensor([ 38, 116, 203,   3,  11,   2,  11,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 38, 419, 203,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 4212 / 7800\tLoss:\t3.702078\n",
            "pred:\t tensor([ 90, 170,  24,   2,  24, 120,   5,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 90, 176,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4368 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([  3, 207,   3,  11,   2,  11,   3,  11,   3,  11])\n",
            "\n",
            "tgt:\t tensor([  3, 207,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4524 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([  3, 354,   5,   2,  11,   3,   3,   3,  11,  11])\n",
            "\n",
            "tgt:\t tensor([251, 354,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4680 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([  3, 314,  11,   2,  11,   3,   3,   3,  11,   3])\n",
            "\n",
            "tgt:\t tensor([  3, 314,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4836 / 7800\tLoss:\t2.728494\n",
            "pred:\t tensor([ 38,  40, 359, 390,  11,   2,  11,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 38,  40,  21, 390,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 4992 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([14, 28,  3, 11,  2, 11,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 5148 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([267,   3,  24,   2,  24, 226,   5,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([267, 234,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 5304 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([52,  3, 11,  2, 11,  3,  3,  3, 11,  3])\n",
            "\n",
            "tgt:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 5460 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([36,  3,  3,  3, 11,  2, 11,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([ 38,  78, 214,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 5616 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([48,  3, 11,  3, 11,  2, 11,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([48,  3, 37, 10, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 5772 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 3, 11, 11,  2, 11,  3, 11,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([  3, 161,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 5928 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 3,  5,  5,  2, 24,  3,  3,  3, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 3, 19,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6084 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([36,  3, 11,  2, 11,  3, 11,  3, 11,  3])\n",
            "\n",
            "tgt:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6240 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([3, 5, 2, 5, 3, 3, 3, 3, 3, 3])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 6396 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([171, 341,  86,  11,   2,  11,   3,  11,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 92, 341, 236,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6552 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([267, 117,  24,   2,  24, 120,   3,   3,  11,  11])\n",
            "\n",
            "tgt:\t tensor([267, 118,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 6708 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([ 14,  17, 114,  11,   2,  11,   3,  11,   3,  11])\n",
            "\n",
            "tgt:\t tensor([ 14,  17, 114,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6864 / 7800\tLoss:\t1.584817\n",
            "pred:\t tensor([14, 79, 28, 41,  3, 11,  2, 11,  3,  3])\n",
            "\n",
            "tgt:\t tensor([ 14,  79,  28,  41, 229,  11,   2,   0,   0])\n",
            "\n",
            "iter 7020 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([267,   3,  24,   2,  24, 120, 229, 144,  11,  11])\n",
            "\n",
            "tgt:\t tensor([267, 124,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 7176 / 7800\tLoss:\t1.970186\n",
            "pred:\t tensor([ 15, 204, 393,   3,  11,   2,  11,   3,  11,   3])\n",
            "\n",
            "tgt:\t tensor([ 15, 204, 393,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 7332 / 7800\tLoss:\t1.661103\n",
            "pred:\t tensor([52,  3, 11,  2, 11,  3,  3,  3, 11,  3])\n",
            "\n",
            "tgt:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 7488 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([15,  3, 11,  2, 11,  3,  3, 11,  3,  3])\n",
            "\n",
            "tgt:\t tensor([15, 23, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 7644 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([14, 28,  3, 11,  2, 11,  3, 11, 11,  3])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def train_rnn(net, train_iter, lr, epochs, device):\n",
        "  # training\n",
        "  net = net.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      loss_list.append(loss.mean().detach().cpu())\n",
        "      optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
        "  return loss_list\n",
        "\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "lr = 1e-3\n",
        "epochs = 50\n",
        "\n",
        "embedding_dim = 250\n",
        "hidden_size = 128\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "rnn_net = NMTRNN(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size)\n",
        "\n",
        "rnn_loss_list = train_rnn(rnn_net, train_iter, lr, epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGs3VUoRjFel"
      },
      "source": [
        "### RNN Loss Curve\n",
        "\n",
        "Plot the loss curve over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fHsMfcssjFel",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "1930ccae-1c1e-4252-800e-50e28f9728f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curve of Baseline')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# save the loss curve figure in a file for the report\n",
        "plt.plot(np.arange(len(rnn_loss_list)), rnn_loss_list)\n",
        "plt.title('Loss Curve of Baseline')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia91raTAjFel"
      },
      "source": [
        "### Prediction Accuracy\n",
        "\n",
        "Print out 5 prediction samples, and calculate the prediction accuracy over the training dataset. You will see an accuracy of over 70%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lbOv1b7YjFel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a54b4cd-d173-49f0-d4a4-975ba8d1c8e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "pred:\t ['unk', '!', '.', '.', '.']\n",
            "\n",
            "tgt:\t ['unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', 'te', 'déteste', '.', '.', '.']\n",
            "\n",
            "tgt:\t ['je', \"t'en\", 'dois', 'une', '.', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['est-il', 'unk', '?', '?', '.', '.']\n",
            "\n",
            "tgt:\t ['est-il', 'unk', '?', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', 'suis', 'unk', '.', '.', '.']\n",
            "\n",
            "tgt:\t ['je', 'suis', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['unk', 'à', 'qui', 'que', 'ce', 'soit', '!', '?']\n",
            "\n",
            "tgt:\t ['demande', 'à', 'unk', 'qui', '!', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "Prediction Acc.: 0.7158\n"
          ]
        }
      ],
      "source": [
        "def comp_acc(pred, gt, valid_len):\n",
        "  N, T_gt = gt.shape[:2]\n",
        "  _, T_pr = pred.shape[:2]\n",
        "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
        "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
        "  len_mask = len_mask < valid_len[:, None]\n",
        "\n",
        "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
        "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
        "  return pred_acc\n",
        "\n",
        "def evaluate_rnn(net, train_iter, device):\n",
        "  acc_list = []\n",
        "  for i, train_data in enumerate(train_iter):\n",
        "    train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "    pred = net.predict(*train_data)\n",
        "\n",
        "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
        "    acc_list.append(pred_acc)\n",
        "    if i < 5:# print 5 samples from 5 batches\n",
        "      pred = pred[0].detach().cpu()\n",
        "      pred_seq = []\n",
        "      for t in range(MAX_LEN+1):\n",
        "        pred_wd = vocab_fra.index2word[pred[t].item()]\n",
        "        if pred_wd != 'eos':\n",
        "          pred_seq.append(pred_wd)\n",
        "\n",
        "      print('pred:\\t {}\\n'.format(pred_seq))\n",
        "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
        "\n",
        "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
        "\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "\n",
        "evaluate_rnn(rnn_net, train_iter, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op1p4D-ujFem"
      },
      "source": [
        "## Part 1b: Sequence to Sequence with LSTM and Attention\n",
        "\n",
        "Now let's try to improve our model by using an LSTM and the attention mechanism.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1S5V9FjFem"
      },
      "source": [
        "### LSTM\n",
        "\n",
        "LSTMs eliminate the gradient explosion/vanishing problem. Its state and gate update at each time step can be summarized as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\\text{State Update} &&& C_t &= F_t \\odot C_{t-1} + I_t \\odot \\tilde{C}_t \\\\\n",
        "&\\text{Hidden States} &&& H_t &= O_t \\odot \\text{tanh}(C_t) \\\\\n",
        "&\\text{Proposal} &&& \\tilde{C}_t &= \\text{tanh}( X_tW_{xc} + H_{t-1}W_{hc} + b_c ) \\\\\n",
        "&\\text{Input Gate} &&& I_t &= \\sigma( X_tW_{xi} + H_{t-1}W_{hi} + b_i ) \\\\\n",
        "&\\text{Forget Gate} &&& F_t &= \\sigma( X_tW_{xf} + H_{t-1}W_{hf} + b_f ) \\\\\n",
        "&\\text{Output Gate} &&& O_t &= \\sigma( X_tW_{xo} + H_{t-1}W_{ho} + b_o ) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Implement the LSTM class below. In particular,\n",
        "-  Complete the initialization function *init_params()*. Weights should be initialized using `torch.randn` multiplied with a scale of 0.1. Biases should be initialized to 0.\n",
        "- Complete the function *lstm()* which performs the feed-forward pass of LSTM. **Do not** use `nn.LSTM` or `nn.LSTMCell` in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "deletable": false,
        "id": "rhPfb8chjFem",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "386a51799571153e76849c4b5ebb7f73",
          "grade": false,
          "grade_id": "cell-e43516618029ca06",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, device):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.device = device\n",
        "    self.params = nn.ParameterList(self.init_params(input_size, hidden_size))\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      input_size: int, feature dimension of input sequence\n",
        "      hidden_size: int, feature dimension of hidden state\n",
        "      device: torch.device()\n",
        "    \"\"\"\n",
        "\n",
        "  def init_params(self, input_size, hidden_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      input_size: int, feature dimension of input sequence\n",
        "      hidden_size: int, feature dimension of hidden state\n",
        "\n",
        "    Outputs:\n",
        "      Weights for proposal: W_xc, W_hc, b_c\n",
        "      Weights for input gate: W_xi, W_hi, b_i\n",
        "      Weights for forget gate: W_xf, W_hf, b_f\n",
        "      Weights for output gate: W_xo, W_ho, b_o\n",
        "    \"\"\"\n",
        "    W_xc, W_hc, b_c = None, None, None\n",
        "    W_xi, W_hi, b_i = None, None, None\n",
        "    W_xf, W_hf, b_f = None, None, None\n",
        "    W_xo, W_ho, b_o = None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize the weights and biases. The result will be stored in\n",
        "    # `params` below. Weights should be initialized using `torch.randn` multiplied\n",
        "    # with the scale (0.1). Biases should be initialized to 0.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code (4 pts)\n",
        "\n",
        "    #0.1 is scale, could make variable but this works\n",
        "    #make nn.Parameter?\n",
        "    W_xc = (torch.randn(input_size, hidden_size) * 0.1)\n",
        "    W_hc = (torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "    b_c = (torch.zeros(hidden_size))\n",
        "\n",
        "    W_xi = (torch.randn(input_size, hidden_size) * 0.1)\n",
        "    W_hi = (torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "    b_i = (torch.zeros(hidden_size))\n",
        "\n",
        "    W_xf = (torch.randn(input_size, hidden_size) * 0.1)\n",
        "    W_hf = (torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "    b_f = (torch.zeros(hidden_size))\n",
        "\n",
        "    W_xo = (torch.randn(input_size, hidden_size) * 0.1)\n",
        "    W_ho = (torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "    b_o = (torch.zeros(hidden_size))\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    params = [W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o]\n",
        "    return params\n",
        "\n",
        "\n",
        "  def lstm(self, X, state):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tuple of tensors (src, src_len). src, size (N, D_in) or (N, T, D_in), where N is the batch size,\n",
        "        T is the length of the sequence(s). src_len, size of (N,), is the valid length for each sequence.\n",
        "\n",
        "      state: tuple of tensors (h, c). h, size of (N, hidden_size) is the hidden state of LSTM. c, size of\n",
        "            (N, hidden_size), is the memory cell of the LSTM.\n",
        "\n",
        "    Outputs:\n",
        "      o: tensor of size (N, T, hidden_size). Contains the output features (the hidden state H_t) for each t.\n",
        "      state: the same as input state. Contains the hidden state H_T and cell state C_T for the last timestep T.\n",
        "    \"\"\"\n",
        "\n",
        "    src, valid_len = X\n",
        "    h, c = state\n",
        "\n",
        "    # make sure always has a T dim\n",
        "    if len(src.shape) == 2:\n",
        "      src = src.unsqueeze(1)\n",
        "\n",
        "    N, T, D_in = src.shape\n",
        "    W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o = self.params\n",
        "    o = []\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of the LSTM.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code (5 pts)\n",
        "    for step in range(T):\n",
        "      x_t = src[:, step, :]\n",
        "\n",
        "      proposal = torch.tanh(x_t @ W_xc + h @ W_hc + b_c)\n",
        "      input_gate = torch.sigmoid(x_t @ W_xi + h @ W_hi + b_i) #values between 0, 1 so use sigmoid not tanh... oops\n",
        "      forget_gate = torch.sigmoid(x_t @ W_xf + h @ W_hf + b_f)\n",
        "      output_gate = torch.sigmoid(x_t @ W_xo + h @ W_ho + b_o)\n",
        "\n",
        "      c = (forget_gate * c) + (input_gate * proposal) #update state c\n",
        "      h = output_gate * torch.tanh(proposal) #update state h\n",
        "\n",
        "      o.append(h.unsqueeze(1))\n",
        "\n",
        "    o = torch.cat(o, dim=1)\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    state = (h, c)\n",
        "    return o, state\n",
        "\n",
        "  def forward(self, inputs, state):\n",
        "    return self.lstm(inputs, state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_Mo2vb1jFem"
      },
      "source": [
        "Check that your output has the correct shape. You should see:\n",
        "\n",
        "```\n",
        "torch.Size([12, 8, 5])\n",
        "torch.Size([12, 5])\n",
        "torch.Size([12, 5])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "B30tskKHjFem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f1c6e3-ae12-43b3-c1dd-af865b5a6af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 8, 5])\n",
            "torch.Size([12, 5])\n",
            "torch.Size([12, 5])\n"
          ]
        }
      ],
      "source": [
        "test_lstm = LSTM(10, 5, torch.device('cpu'))\n",
        "test_src = torch.ones(12, 8, 10)\n",
        "test_src_len = torch.ones(12) * 8\n",
        "test_h = torch.zeros(12, 5).float()\n",
        "test_c = torch.zeros(12, 5).float()\n",
        "\n",
        "test_o, test_state = test_lstm((test_src, test_src_len), (test_h, test_c))\n",
        "\n",
        "print(test_o.shape)\n",
        "print(test_state[0].shape)\n",
        "print(test_state[1].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB3Ots72jFem"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEqiOnvXjFen"
      },
      "source": [
        "Another improvement we can make to our model is the Attention Mechanism. An example illustrating why applying attention mechanisms can improve the performance is shown in the picture below. An English sentence and its Chinese is visualized and aligned into blue boxes and red boxes, respectively. It can be seen that the Chinese character '她' has a long distance from its English counterpart, 'she'. Since only the final hidden state is passed to the decoder, it's hard for the baseline model to 'attend' to information a long time ago."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2k1WLJ0jFer"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png\" width=\"600\"/>\n",
        "</div>\n",
        "Image source: https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoRDfmoHjFer"
      },
      "source": [
        "- **Attention**\n",
        "\n",
        "    Given a query, $\\mathbf{q} \\in R^{d_q}$, and a set of $N$ (key, value) pairs, $\\{ \\mathbf{k}_i, \\mathbf{v}_i\\}^N$ where $k_i \\in R^{d_k}$ and $v_i \\in R^{d_v}$, the attention mechanism computes a weighted sum of values based on the normalized score obtained from the query and each key:\n",
        "    \n",
        "    \\begin{align*}\n",
        "    a_i &= \\alpha(\\mathbf{q}, \\mathbf{k_i}) \\\\\n",
        "    \\mathbf{a} &= [a_1, ..., a_n] \\\\\n",
        "    \\mathbf{b} &= \\text{softmax}(\\mathbf{a}) \\\\\n",
        "    \\mathbf{o} &= \\mathbf{b} \\cdot \\mathbf{V}\\text{, where } \\mathbf{V} = \\{\\mathbf{v}_i\\}^N\n",
        "    \\end{align*}\n",
        "    \n",
        "    The $\\alpha()$ function, which maps two vectors into a scalar, is the score function that can be chosen from a wide range of functions: e.g. the cosine function, dot-product function, scaled dot-product funtion and etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi3UjQ_PjFer"
      },
      "source": [
        "- **Masked Softmax**\n",
        "\n",
        "For our machine translation task, the inputs and outputs may be of variable length (ie. each training example may have a different number of words). As shown above, we pad our inputs with a special `pad` token so that they all have the same length to make them easier to work with. However, when we take the softmax, we only want to include the non-`pad` items, so we need to write a special `masked_softmax` function to handle this. We can achieve the masking by setting masked elements to a large negative value. Then when we take the `exp`, those elements will be 0 and won't contribute to the softmax. We provide the implementation of this for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wR5lGdsYjFer"
      },
      "outputs": [],
      "source": [
        "def masked_softmax(X, valid_length):\n",
        "  \"\"\"\n",
        "  inputs:\n",
        "    X: 3-D tensor\n",
        "    valid_length: 1-D or 2-D tensor\n",
        "  \"\"\"\n",
        "  mask_value = -1e7\n",
        "\n",
        "  if len(X.shape) == 2:\n",
        "    X = X.unsqueeze(1)\n",
        "\n",
        "  N, n, m = X.shape\n",
        "\n",
        "  if len(valid_length.shape) == 1:\n",
        "    valid_length = valid_length.repeat_interleave(n, dim=0)\n",
        "  else:\n",
        "    valid_length = valid_length.reshape((-1,))\n",
        "\n",
        "  mask = torch.arange(m)[None, :].to(X.device) >= valid_length[:, None]\n",
        "  X.view(-1, m)[mask] = mask_value\n",
        "\n",
        "  Y = torch.softmax(X, dim=-1)\n",
        "\n",
        "\n",
        "  return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Sn1KAbprjFer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62d14ef-053e-4bba-b47f-617a5b433362"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4667, 0.5333, 0.0000, 0.0000],\n",
              "         [0.5474, 0.4526, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.2324, 0.5569, 0.2107, 0.0000],\n",
              "         [0.3379, 0.4132, 0.2489, 0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFcl4VxrjFer"
      },
      "source": [
        "- **Scaled Dot Product Attention**\n",
        "The scaled dot-product attention uses the score function as: $ \\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q} \\mathbf{k}^T / \\sqrt{d} $, where $d$ is the dimension of query (which in this case is equal to the dimension of the keys). The following figures visualizes this process in matrix form, in which $Q \\in \\mathcal{R}^{m\\times d_k}, \\mathbf{K} \\in \\mathcal{R}^{n \\times d_k}$, and $\\mathbf{V} \\in \\mathcal{R}^{n \\times d_v}$.\n",
        "\n",
        "    <div>\n",
        "    <img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width=\"600\"/>\n",
        "    </div>\n",
        "Image source: http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\n",
        "\n",
        "Implement the DotProductAttention below. Do not use any loops in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "deletable": false,
        "id": "fH6x2oLTjFes",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8b7a6c1703c6c230a006a6b86326a3b9",
          "grade": false,
          "grade_id": "cell-eac4fccbcd4f068e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(DotProductAttention, self).__init__()\n",
        "\n",
        "  def forward(self, query, key, value, valid_length=None):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      query: tensor of size (B, n, d)\n",
        "      key: tensor of size (B, m, d)\n",
        "      value: tensor of size (B, m, dim_v)\n",
        "      valid_length: (B, )\n",
        "\n",
        "      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
        "      d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
        "\n",
        "    Outputs:\n",
        "      attention: tensor of size (B, n, dim_v), weighted sum of values\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of DotProductAttention. Do not\n",
        "    # use any loops in your implementation.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code (4 pts)\n",
        "    a = torch.bmm(query, key.transpose(1,2)) / torch.sqrt(torch.tensor(query.size(-1), dtype=torch.float32)) #include dtype I think for error catching\n",
        "    b = masked_softmax(a, valid_length)\n",
        "    attention = torch.bmm(b, value)\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wozEYxrzjFes"
      },
      "source": [
        "### Correctness Check for DotProductAttention\n",
        "\n",
        "Run the following snippet to check your implementation of DotProductAttention.\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "tensor([[[ 2.0000,  3.0000, 4.0000, 5.0000]],\n",
        "\n",
        "        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mHgHg92BjFes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "357a0117-cec6-4a59-804c-b03568a39803"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
              "\n",
              "        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "att = DotProductAttention()\n",
        "keys = torch.ones((2,10,2),dtype=torch.float)\n",
        "values = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\n",
        "att(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tgmXfoXjFes"
      },
      "source": [
        "- **MLP Attention**\n",
        "\n",
        "  In MLP attention, we project both query and keys into $R^h$, add the results, and use a $\\text{tanh}$ before multiplying by the values. The score function is defined as:\n",
        "  \n",
        "  $\\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{v}^T\\text{tanh}(W_k\\mathbf{k} + W_q\\mathbf{q})$\n",
        "  \n",
        "  where $\\mathbf{v}, \\mathbf{W_k}\\text{, and }\\mathbf{W_q}$ are learnable parameters.\n",
        "    \n",
        "Implement the MLP attention in matrix form without using any loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "deletable": false,
        "id": "T9asEUm-jFes",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9440f3519ad5f8037f192758aecca64a",
          "grade": false,
          "grade_id": "cell-6be727894d4fd817",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class MLPAttention(nn.Module):\n",
        "  def __init__(self, d_v, d_k, d_q):\n",
        "    super(MLPAttention, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_k: feature dimension of key\n",
        "      d_v: feature dimension of vector v\n",
        "      d_q: feature dimension of query\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize learnable parameters\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code (1.5 pts)\n",
        "    #Did not use weight? We did above when initializing but did not say to this time, else you should multiply by 0.1\n",
        "    self.v = nn.Linear(d_v, 1) #used self. because __init__ function\n",
        "    self.W_k = nn.Linear(d_k, d_v)\n",
        "    self.W_q = nn.Linear(d_q, d_v)\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "  def forward(self, query, key, value, valid_length):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      query: tensor of size (B, n, d)\n",
        "      key: tensor of size (B, m, d)\n",
        "      value: tensor of size (B, m, dim_v)\n",
        "      valid_length: either (B, )\n",
        "\n",
        "      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
        "      d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
        "\n",
        "    Outputs:\n",
        "      attention: tensor of size (B, n, dim_v), weighted sum of values\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of MLPAttention. Do not\n",
        "    # use any loops in your implementation.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code (3 pts)\n",
        "    \"\"\"k = torch.matmul(key, self.W_k)\n",
        "    q = torch.matmul(query, self.W_q)\"\"\"\n",
        "\n",
        "    k = self.W_k(key)\n",
        "    q = self.W_q(query)\n",
        "\n",
        "    # k: (B, m, d_v) -> (B, 1, m, d_v)\n",
        "    # q: (B, n, d_v) -> (B, n, 1, d_v)\n",
        "    # Result: (B, n, m, d_v)\n",
        "\n",
        "    #a = torch.matmul(torch.tanh(k[:, None, :, :] + q[:, :, None, :]), self.v).squeeze(-1)\n",
        "    a = torch.tanh(q.unsqueeze(2) + k.unsqueeze(1)) #help from a friend\n",
        "\n",
        "    Y = self.v(a)\n",
        "    Y = masked_softmax(Y.squeeze(3), valid_length)\n",
        "    Y = torch.bmm(Y, value)\n",
        "    #error is here\n",
        "    # END OF YOUR CODE\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECgF4ZQOjFes"
      },
      "source": [
        "### Correctness Check for MLPAttention\n",
        "\n",
        "Run the following snippet to check your implementation of MLPAttention.\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "tensor([[[ 2.0000,  3.0000, 4.0000, 5.0000]],\n",
        "\n",
        "        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "60SCdL_3jFes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b51b1c-ae98-4cdf-df9f-165c76909f0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
              "\n",
              "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "atten = MLPAttention(4, 2, 2)\n",
        "atten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6])) #changed it to 2,1,2 because was causing error (list 2x thickness)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUzIwnp3jFet"
      },
      "source": [
        "    \n",
        "- **Using Attention in seq2seq Models**\n",
        "\n",
        "    <div>\n",
        "    <img src=\"https://d2l.ai/_images/seq2seq-attention.svg\" width=\"600\"/>\n",
        "    </div>\n",
        "Image source: https://d2l.ai/_images/seq2seq-attention.svg\n",
        "\n",
        "    Now we want to add attention to the seq2seq model. As we previously stated, attention allows the decoder to have more direct access to previous states in the encoder. In the context of machine translation, when the decoder is predicting a word in the translation, it can focus on certain words in the original language. Therefore, we want the keys and the values of the attention layer to be the output of the encoder at each step. The query for the attention layer would be the decoder's previous hidden state. The output of the attention layer, referred to as the context, is concatenated with the decoder input and fed into the decoder.\n",
        "    \n",
        "    In rough pseudocode, this looks like:\n",
        "    ```\n",
        "    context = attention(query=h_prev, keys=encoder_output, values=encoder_output)\n",
        "    decoder_input = concatenate([decoder_input, context])\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS_dNUx1jFet"
      },
      "source": [
        "### LSTM Encoder-Decoder\n",
        "\n",
        "\n",
        "Build a seq2seq model with LSTM and attention.\n",
        "\n",
        "- Complete the Encoder forward() function.\n",
        "- Complete the Decoder forward() and predict() functions. The decoder should utilize the attention mechanism.\n",
        "- Find a good learning rate for training this model. Feel free to add code here to test out different learning rates, but make sure that your best model is saved in `lstm_net`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": false,
        "id": "OS66DYwVjFet",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fbd2ff0f838eab4eeb305bd07bbd3a8e",
          "grade": false,
          "grade_id": "cell-85d8bda82bc92dd8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(Encoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of vallina RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = LSTM(embedding_dim, hidden_size, device)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, sources, valid_len):\n",
        "    ##############################################################################\n",
        "    # TODO: Implement LSTM Encoder forward pass\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code (2 pts)\n",
        "    word_embedded = self.embedding(sources)\n",
        "    word_embedded = word_embedded.type(torch.float32) #error catching\n",
        "    N = word_embedded.shape[0]\n",
        "\n",
        "    h = sources.new_zeros((N, self.hidden_size), dtype=torch.float32, device=sources.device) #adding device to fix cpu and cuda error\n",
        "    c = sources.new_zeros((N, self.hidden_size), dtype=torch.float32, device=sources.device)\n",
        "\n",
        "    outputs, (h, c) = self.enc((word_embedded, valid_len), (h, c))\n",
        "    # END OF YOUR CODE\n",
        "    return outputs, (h, c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "deletable": false,
        "id": "SNJ8yU9EjFet",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d8f15ad91df74611a4f70744a202ad53",
          "grade": false,
          "grade_id": "cell-154ce877082ed913",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(Decoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of vallina RNN\n",
        "    \"\"\"\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = LSTM(embedding_dim+hidden_size, hidden_size, device)\n",
        "    self.att = MLPAttention(d_v=hidden_size, d_k=hidden_size, d_q=hidden_size)\n",
        "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, state, target, valid_len):\n",
        "    loss = 0\n",
        "    preds = []\n",
        "\n",
        "    ##############################################################################\n",
        "    # TODO: Implement LSTM Decoder forward pass. Your solution should also use\n",
        "    # self.att for attention.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code (4 pts)\n",
        "    encoder_output, (h, c), length = state\n",
        "    embeds = self.embedding(target)\n",
        "    n, T = target.shape\n",
        "\n",
        "    for t in range(T - 1):\n",
        "      context = self.att(h.unsqueeze(1), encoder_output, encoder_output, length)\n",
        "      decoder_input = torch.concat([embeds[:,t].unsqueeze(1), context], dim=2)\n",
        "\n",
        "      o, (h, c) = self.enc((decoder_input, valid_len), (h,c))\n",
        "      out = self.output_emb(o.squeeze(1))\n",
        "      preds.append(out)\n",
        "\n",
        "      loss = loss + F.nll_loss(F.log_softmax(out, dim=1), target[:, t+1], ignore_index=0) #got from above, my loss didn't work but this has nans so\n",
        "\n",
        "    pred = torch.stack(preds, dim = 1)\n",
        "    # END OF YOUR CODE\n",
        "    return loss, preds\n",
        "\n",
        "  def predict(self, state, target, valid_len):\n",
        "    pred = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement LSTM Encoder prediction. Your solution should also use\n",
        "    # self.att for attention.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code (4 pts)\n",
        "    preds = []\n",
        "    encoder_output, (h,c), length = state\n",
        "    n, T = target.shape\n",
        "    input = target[:,0]\n",
        "\n",
        "    for t in range(1, target.size(1)):\n",
        "      target_emb = self.embedding(input).unsqueeze(1)\n",
        "      attention = self.att(h.unsqueeze(1), encoder_output, encoder_output, length)\n",
        "      input = torch.cat([target_emb, attention], dim = 2)\n",
        "\n",
        "      o, (h,c) = self.enc((input, valid_len), (h,c))\n",
        "      pred = self.output_emb(o.squeeze(1))\n",
        "      input = pred.argmax(dim = 1)\n",
        "      preds.append(input)\n",
        "\n",
        "    pred = torch.stack(preds,dim = 1)\n",
        "    # END OF YOUR CODE\n",
        "    return pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0dlMsFlXjFet"
      },
      "outputs": [],
      "source": [
        "class NMTLSTM(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(NMTLSTM, self).__init__()\n",
        "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size, device)\n",
        "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size, device)\n",
        "\n",
        "  def forward(self, src, src_len, tgt, tgt_len):\n",
        "    outputs, (h, c) = self.enc(src, src_len)\n",
        "    loss, pred = self.dec((outputs, (h, c), src_len), tgt, tgt_len)\n",
        "    return loss, pred\n",
        "\n",
        "  def predict(self, src, src_len, tgt, tgt_len):\n",
        "    outputs, (h, c) = self.enc(src, src_len)\n",
        "    pred = self.dec.predict((outputs, (h, c), src_len), tgt, tgt_len)\n",
        "    return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "deletable": false,
        "id": "uT6kUHk2jFet",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "73bc6101f369326d21d0efe8439c652b",
          "grade": false,
          "grade_id": "cell-bfaaa623c7199b2d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5550c8bf-0701-4162-b665-c2f3bd14f2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "iter 0 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-0.1928,  0.4104, -0.1086,  ...,  0.2304, -0.0253, -0.1600],\n",
            "        [-0.2375,  0.4713, -0.0785,  ...,  0.2005, -0.0209, -0.2314],\n",
            "        [-0.1341,  0.3979, -0.0967,  ...,  0.2509, -0.0169, -0.1639],\n",
            "        ...,\n",
            "        [-0.1110,  0.4234, -0.1319,  ...,  0.1931, -0.0287, -0.1488],\n",
            "        [-0.1766,  0.4168, -0.0804,  ...,  0.1957, -0.0403, -0.2039],\n",
            "        [-0.1956,  0.4684, -0.1231,  ...,  0.2493, -0.0008, -0.1645]])\n",
            "\n",
            "tgt:\t tensor([ 92, 341,   3,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 156 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-2.9213, -2.3143,  0.6090,  ..., -1.9828, -1.3126, -1.9369],\n",
            "        [-2.8938, -2.2838,  0.5707,  ..., -1.9623, -1.3005, -1.9131],\n",
            "        [-2.8508, -2.2500,  0.5262,  ..., -1.9394, -1.2826, -1.8943],\n",
            "        ...,\n",
            "        [-2.8532, -2.2550,  0.5293,  ..., -1.9295, -1.2789, -1.8909],\n",
            "        [-2.8344, -2.2438,  0.5128,  ..., -1.9265, -1.2747, -1.8944],\n",
            "        [-2.9213, -2.3143,  0.6090,  ..., -1.9828, -1.3126, -1.9369]])\n",
            "\n",
            "tgt:\t tensor([155, 357, 254,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 312 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-3.5371, -2.9482,  0.2118,  ..., -1.8110, -0.6572, -2.1363],\n",
            "        [-3.5829, -2.9822,  0.2507,  ..., -1.8256, -0.6898, -2.1400],\n",
            "        [-3.5008, -2.9379,  0.1230,  ..., -1.7688, -0.6198, -2.1206],\n",
            "        ...,\n",
            "        [-2.4591, -2.2246,  0.2302,  ..., -1.0016, -0.0464, -1.6720],\n",
            "        [-3.6693, -3.0309,  0.3797,  ..., -1.9060, -0.7435, -2.1683],\n",
            "        [-3.6303, -3.0045,  0.3511,  ..., -1.8842, -0.7270, -2.1563]])\n",
            "\n",
            "tgt:\t tensor([ 38,  40, 359,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 468 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-3.6820, -2.7485, -0.1592,  ..., -1.3702,  0.0067, -2.0367],\n",
            "        [-3.7760, -2.7231, -0.2814,  ..., -1.1977,  0.0986, -1.9406],\n",
            "        [-3.9617, -3.1568,  0.2566,  ..., -1.6499, -0.2193, -2.2334],\n",
            "        ...,\n",
            "        [-4.1172, -3.3441,  0.3034,  ..., -1.7198, -0.2933, -2.2875],\n",
            "        [-3.9811, -3.1757,  0.1752,  ..., -1.6136, -0.2126, -2.2118],\n",
            "        [-4.2250, -3.4584,  0.3607,  ..., -1.7895, -0.3450, -2.3323]])\n",
            "\n",
            "tgt:\t tensor([155, 157, 334,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 624 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.1867, -3.2864, -0.1628,  ..., -1.1068,  0.2476, -2.0402],\n",
            "        [-4.0190, -3.1369, -0.2021,  ..., -1.0530,  0.2754, -1.9624],\n",
            "        [-4.5321, -3.9275,  0.4093,  ..., -1.7527, -0.3925, -2.4143],\n",
            "        ...,\n",
            "        [-2.6548, -2.4851, -0.1543,  ..., -1.1787,  0.2126, -1.6434],\n",
            "        [-4.4695, -3.7313,  0.2801,  ..., -1.6121, -0.2009, -2.3425],\n",
            "        [-4.5176, -3.9107,  0.3909,  ..., -1.7530, -0.3989, -2.4208]])\n",
            "\n",
            "tgt:\t tensor([250,  75,   3,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 780 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.2248, -3.8116,  0.3574,  ..., -1.4036, -0.2196, -1.8691],\n",
            "        [-4.6807, -3.9169, -0.1286,  ..., -1.0271,  0.1032, -2.2080],\n",
            "        [-3.7276, -3.1664, -0.2689,  ..., -0.9951,  0.4805, -1.8104],\n",
            "        ...,\n",
            "        [-4.4476, -3.6420, -0.2598,  ..., -0.7497,  0.3910, -2.0142],\n",
            "        [-4.7894, -4.2754,  0.3645,  ..., -1.6639, -0.3694, -2.5263],\n",
            "        [-4.7024, -4.1644,  0.3403,  ..., -1.6295, -0.3375, -2.4923]])\n",
            "\n",
            "tgt:\t tensor([ 3,  3, 24,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 936 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.5619, -3.7494, -0.4392,  ..., -0.8404,  0.6618, -2.2429],\n",
            "        [-4.3711, -4.0431,  0.4385,  ..., -1.4867,  0.0837, -1.7960],\n",
            "        [-4.8252, -3.9832, -0.4174,  ..., -0.8254,  0.5700, -2.2486],\n",
            "        ...,\n",
            "        [-4.1711, -3.5425, -0.5585,  ..., -0.5774,  0.7965, -1.8542],\n",
            "        [-2.7669, -2.6264, -0.2372,  ..., -1.2557,  0.2653, -1.5370],\n",
            "        [-4.2507, -3.6059, -0.4249,  ..., -0.6876,  0.7942, -1.8483]])\n",
            "\n",
            "tgt:\t tensor([ 52, 358,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1092 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-2.9786, -2.6336, -0.5532,  ..., -0.3973,  1.5588, -1.7096],\n",
            "        [-4.9491, -4.5365,  0.2411,  ..., -1.6949,  0.1639, -2.6231],\n",
            "        [-2.6776, -2.4860, -0.3892,  ..., -1.0386,  0.4499, -1.3557],\n",
            "        ...,\n",
            "        [-4.7511, -4.3160,  0.0568,  ..., -1.5049,  0.2080, -2.4465],\n",
            "        [-4.9763, -4.5608,  0.2426,  ..., -1.7105,  0.1583, -2.6371],\n",
            "        [-4.1222, -3.6658, -0.5118,  ..., -0.8521,  0.9396, -2.0910]])\n",
            "\n",
            "tgt:\t tensor([318, 171, 319, 107, 249,  11,   2,   0,   0])\n",
            "\n",
            "iter 1248 / 7800\tLoss:\t7.141070\n",
            "pred:\t tensor([[-4.2984, -3.7291, -0.6413,  ...,  0.0063,  1.0994, -1.8017],\n",
            "        [-4.2550, -3.7304, -0.4639,  ...,  0.1334,  1.1540, -1.7493],\n",
            "        [-4.2084, -4.0042, -0.0090,  ..., -1.6674,  0.3084, -2.3328],\n",
            "        ...,\n",
            "        [-4.4092, -4.0022, -0.2396,  ..., -1.3559,  0.8218, -2.5763],\n",
            "        [-4.0449, -3.5713, -0.4958,  ..., -1.1598,  1.1694, -2.4787],\n",
            "        [-5.1047, -4.5673, -0.3404,  ..., -0.1594,  0.9291, -1.7498]])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 1404 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-5.3228, -5.0572,  0.3864,  ..., -1.2053,  0.9716, -2.3546],\n",
            "        [-4.6193, -4.1580, -0.0959,  ..., -1.8598, -0.0235, -2.1431],\n",
            "        [-3.6697, -3.0706, -0.9298,  ..., -1.1343,  1.2399, -2.2492],\n",
            "        ...,\n",
            "        [-4.0864, -3.4592, -1.1455,  ...,  0.3566,  0.9503, -1.4612],\n",
            "        [-2.7607, -2.5755, -0.6347,  ..., -0.8018,  1.1698, -1.9216],\n",
            "        [-5.6365, -5.0664,  0.2535,  ..., -2.0321, -0.2192, -2.7491]])\n",
            "\n",
            "tgt:\t tensor([15,  3,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 1560 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.4781, -4.0558, -0.3521,  ..., -1.2683,  0.3190, -1.7819],\n",
            "        [-2.7626, -2.4988, -1.1151,  ..., -1.3064,  1.0922, -2.3355],\n",
            "        [-5.2436, -4.8206, -0.2314,  ..., -1.2038,  0.3224, -2.4426],\n",
            "        ...,\n",
            "        [-2.5889, -2.6532, -0.9106,  ...,  1.0318,  2.1847, -1.5017],\n",
            "        [-5.7720, -5.3146, -0.3606,  ..., -0.7849,  0.5818, -1.8553],\n",
            "        [-5.5510, -5.0049,  0.0444,  ..., -1.7192,  0.1388, -2.5204]])\n",
            "\n",
            "tgt:\t tensor([ 14, 116,  28, 247,  34, 367,  11,   2,   0])\n",
            "\n",
            "iter 1716 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-5.1513e+00, -4.8586e+00,  2.2938e-01,  ..., -1.5482e+00,\n",
            "          8.7769e-02, -2.5745e+00],\n",
            "        [-5.9515e+00, -5.5076e+00, -5.4468e-01,  ..., -3.6844e-01,\n",
            "          5.2614e-01, -1.7570e+00],\n",
            "        [-4.8139e+00, -4.4941e+00, -1.5074e-03,  ..., -1.5594e+00,\n",
            "         -6.8552e-03, -2.3813e+00],\n",
            "        ...,\n",
            "        [-6.0000e+00, -5.5286e+00, -3.6213e-01,  ..., -3.8040e-01,\n",
            "          2.9940e-01, -1.5737e+00],\n",
            "        [-4.1190e+00, -3.9313e+00, -4.1966e-01,  ..., -1.4959e+00,\n",
            "          1.2539e-01, -2.2817e+00],\n",
            "        [-6.2026e+00, -5.7421e+00, -2.0783e-01,  ..., -9.7264e-01,\n",
            "          3.6032e-01, -1.9993e+00]])\n",
            "\n",
            "tgt:\t tensor([15, 75,  3,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 1872 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-5.2774, -4.8407, -0.7776,  ...,  0.4167,  0.8854, -1.3427],\n",
            "        [-6.4393, -6.0328, -0.1700,  ..., -1.3376, -0.2215, -2.3916],\n",
            "        [-3.1147, -2.9267, -0.9870,  ..., -0.8191,  1.4427, -2.2249],\n",
            "        ...,\n",
            "        [-4.1466, -3.8272, -0.5262,  ..., -0.0500,  2.2585, -2.2304],\n",
            "        [-4.6754, -4.1619, -0.8368,  ..., -1.4999,  1.0650, -2.5378],\n",
            "        [-5.9217, -5.4593, -0.3293,  ..., -0.0202,  0.5826, -1.4221]])\n",
            "\n",
            "tgt:\t tensor([ 3, 56,  3,  5,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 2028 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-5.7026, -5.1355,  0.2664,  ..., -1.8408, -0.4048, -2.4116],\n",
            "        [-6.1134, -5.4133,  0.0204,  ..., -1.7116, -0.5957, -2.2361],\n",
            "        [-4.4377, -3.8437, -0.2553,  ..., -1.1206,  0.2235, -2.6427],\n",
            "        ...,\n",
            "        [-5.2141, -4.5779,  0.0553,  ..., -1.2357,  0.3581, -1.6066],\n",
            "        [-3.9564, -3.4428, -0.6236,  ..., -1.4911,  1.0964, -2.5385],\n",
            "        [-5.8565, -5.0824, -1.1651,  ..., -0.2006,  0.7421, -1.7080]])\n",
            "\n",
            "tgt:\t tensor([14,  3,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 2184 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.6888, -4.0801, -1.0437,  ..., -0.8852,  2.0901, -2.7716],\n",
            "        [-6.2709, -5.9310, -0.8236,  ..., -0.5817,  0.7659, -1.6306],\n",
            "        [-4.7127, -4.3964, -0.4027,  ..., -1.7949,  0.5598, -2.5285],\n",
            "        ...,\n",
            "        [-4.8692, -4.6025, -0.9962,  ...,  1.1926,  1.0950, -1.3720],\n",
            "        [-5.2503, -4.9858, -0.8719,  ...,  1.1276,  0.7694, -1.2646],\n",
            "        [-5.5228, -5.1198, -0.9370,  ...,  0.6539,  0.7609, -1.4014]])\n",
            "\n",
            "tgt:\t tensor([ 38,  40, 324,  37, 353,  11,   2,   0,   0])\n",
            "\n",
            "iter 2340 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.6589, -4.0091, -1.0218,  ..., -0.8110,  2.2670, -2.9985],\n",
            "        [-4.2202, -3.5565, -0.8377,  ..., -1.9663, -0.0973, -2.1346],\n",
            "        [-5.5895, -5.0029, -0.4888,  ..., -1.2814,  0.2921, -2.2139],\n",
            "        ...,\n",
            "        [-6.2857, -5.6649, -0.0155,  ..., -1.5828, -0.2493, -2.4119],\n",
            "        [-4.9131, -4.7684, -1.1038,  ...,  0.8038,  2.1601, -1.8773],\n",
            "        [-4.3758, -4.0749, -1.2226,  ...,  1.4868,  2.1002, -1.2828]])\n",
            "\n",
            "tgt:\t tensor([38, 40,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 2496 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.1315e+00, -3.7600e+00, -2.0918e-01,  ..., -3.5226e-01,\n",
            "          1.1754e+00, -2.2576e+00],\n",
            "        [-4.1561e+00, -3.7312e+00, -5.3400e-01,  ..., -2.4042e+00,\n",
            "         -1.4526e-01, -2.1615e+00],\n",
            "        [-3.4006e+00, -3.2045e+00, -8.8353e-01,  ..., -1.6163e-01,\n",
            "          2.8927e+00, -3.1587e+00],\n",
            "        ...,\n",
            "        [-4.6530e+00, -4.2613e+00, -1.2582e+00,  ...,  5.3666e-02,\n",
            "          1.5431e+00, -1.9051e+00],\n",
            "        [-3.2027e+00, -2.6284e+00, -7.6222e-01,  ..., -2.1440e+00,\n",
            "         -1.4476e-03, -1.7895e+00],\n",
            "        [-5.1002e+00, -4.8440e+00, -8.5913e-01,  ...,  1.2721e+00,\n",
            "          1.3070e+00, -1.0220e+00]])\n",
            "\n",
            "tgt:\t tensor([ 62, 107,  14, 171, 191,  11,   2,   0,   0])\n",
            "\n",
            "iter 2652 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-6.1980, -5.7685, -0.9567,  ...,  1.0206,  0.6582, -1.0912],\n",
            "        [-3.0068, -2.4813, -1.0610,  ..., -0.7054,  1.7116, -2.1684],\n",
            "        [-6.2431, -5.4549, -0.9999,  ...,  0.3163,  1.1713, -1.0192],\n",
            "        ...,\n",
            "        [-5.2147, -4.4737, -0.4757,  ..., -1.4423,  0.5434, -1.9882],\n",
            "        [-6.0925, -5.5776, -1.2170,  ...,  1.0448,  0.7081, -0.8988],\n",
            "        [-6.3555, -5.6292, -0.0221,  ..., -1.3510, -0.5217, -1.4974]])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 2808 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.9797, -4.5149, -0.6802,  ..., -1.1076,  1.3081, -2.2169],\n",
            "        [-6.0369, -5.6017, -1.0422,  ..., -0.3530,  0.3609, -1.3084],\n",
            "        [-5.4702, -4.9760, -0.6220,  ..., -1.0927,  0.0472, -1.3976],\n",
            "        ...,\n",
            "        [-3.5528, -3.0741, -1.9050,  ...,  1.8114,  2.0041, -1.7996],\n",
            "        [-4.6912, -4.2011, -0.7592,  ..., -1.0233,  1.2956, -2.1244],\n",
            "        [-4.6424, -4.1260,  0.5283,  ..., -2.1108,  1.2282, -1.8752]])\n",
            "\n",
            "tgt:\t tensor([38, 40,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 2964 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-3.1652, -2.7405, -1.1628,  ..., -1.0677,  2.0805, -2.9481],\n",
            "        [-6.7150, -6.1133, -0.1521,  ..., -1.2256, -0.4778, -1.7301],\n",
            "        [-5.1076, -4.6527, -1.7909,  ...,  1.5824,  1.1788, -1.3913],\n",
            "        ...,\n",
            "        [-1.7865, -0.9775, -1.7282,  ..., -1.7932,  1.4258, -2.2518],\n",
            "        [-6.0218, -5.4533,  0.0965,  ..., -1.7871,  0.0881, -2.5884],\n",
            "        [-6.6915, -6.0310, -0.0611,  ..., -1.4921, -0.4213, -2.5142]])\n",
            "\n",
            "tgt:\t tensor([171, 342,  45,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3120 / 7800\tLoss:\t3.258815\n",
            "pred:\t tensor([[-5.1203e+00, -4.5281e+00,  3.5489e-01,  ..., -1.4273e+00,\n",
            "          1.3236e+00, -1.6709e+00],\n",
            "        [-2.6871e+00, -2.5051e+00, -7.2100e-01,  ...,  5.1958e-01,\n",
            "          3.5483e+00, -2.6305e+00],\n",
            "        [-2.1495e+00, -1.4766e+00, -8.3383e-01,  ..., -1.4630e+00,\n",
            "         -6.1634e-03, -1.2830e+00],\n",
            "        ...,\n",
            "        [-5.1036e+00, -4.4722e+00,  3.6294e-01,  ..., -1.4389e+00,\n",
            "          1.2633e+00, -1.6992e+00],\n",
            "        [-6.6550e+00, -5.9191e+00, -2.8647e-01,  ..., -8.9211e-01,\n",
            "         -5.5373e-01, -1.4143e+00],\n",
            "        [-4.9425e+00, -4.4084e+00, -6.0347e-01,  ..., -1.7452e+00,\n",
            "         -1.1893e-01, -1.7843e+00]])\n",
            "\n",
            "tgt:\t tensor([176, 207, 184,   3,  24,   2,   0,   0,   0])\n",
            "\n",
            "iter 3276 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-5.5947, -5.1601, -0.4778,  ..., -0.6928,  0.7642, -2.0388],\n",
            "        [-4.6839, -4.2370,  0.1128,  ..., -2.2599, -0.4323, -1.9662],\n",
            "        [-3.8808, -3.3327, -0.8810,  ..., -1.3823,  1.0636, -2.6082],\n",
            "        ...,\n",
            "        [-2.2871, -2.3824, -1.5124,  ...,  0.1480,  3.0222, -2.5365],\n",
            "        [-3.4550, -3.2062, -1.4164,  ...,  1.5012,  2.8477, -1.6756],\n",
            "        [-2.9028, -2.4854, -2.3242,  ...,  1.8624,  1.9289, -1.1539]])\n",
            "\n",
            "tgt:\t tensor([ 38, 338,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3432 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-5.9989, -5.4465, -0.7030,  ..., -0.9395, -0.2244, -1.3557],\n",
            "        [-4.9472, -4.2699,  0.3255,  ..., -1.3222,  1.4506, -1.9376],\n",
            "        [-6.3447, -6.0658, -0.1255,  ...,  0.7690,  2.7154, -2.6043],\n",
            "        ...,\n",
            "        [-4.6965, -3.6345, -0.6661,  ..., -0.7240,  1.2977, -0.9110],\n",
            "        [-3.4408, -2.7705, -2.4758,  ..., -0.4084,  1.3085, -1.3524],\n",
            "        [-4.8141, -4.2157, -0.3322,  ..., -1.4174, -0.5951, -1.2049]])\n",
            "\n",
            "tgt:\t tensor([ 14, 246,  74,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 3588 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-6.0159, -5.3869, -1.0254,  ..., -0.6760,  2.4925, -3.6036],\n",
            "        [-4.3585, -4.4135, -0.2571,  ..., -0.2675,  1.4708, -2.8106],\n",
            "        [-4.1756, -4.0098, -1.3951,  ...,  1.5813,  2.5054, -1.7429],\n",
            "        ...,\n",
            "        [-6.6951, -6.0412,  0.0366,  ..., -1.0724, -0.3088, -1.5341],\n",
            "        [-5.7786, -5.2588, -0.6144,  ..., -1.5615,  0.6290, -1.8695],\n",
            "        [-3.8701, -3.6767, -1.6349,  ...,  0.0891,  3.0587, -3.2548]])\n",
            "\n",
            "tgt:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 3744 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-5.0999, -4.3971, -1.8069,  ...,  0.5249,  1.4184, -1.2450],\n",
            "        [-5.2326, -4.7684, -0.2630,  ..., -1.5735,  0.4758, -1.9059],\n",
            "        [-5.1837, -5.0354, -0.9022,  ...,  1.7150,  2.5824, -1.9005],\n",
            "        ...,\n",
            "        [-5.7770, -5.4243, -0.5449,  ..., -0.7263,  0.2267, -1.3463],\n",
            "        [-5.4027, -4.8435, -0.5369,  ..., -1.0454,  0.2900, -2.1388],\n",
            "        [-4.9218, -4.6449,  1.3254,  ..., -2.0280,  0.9936, -1.3697]])\n",
            "\n",
            "tgt:\t tensor([135,  72,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3900 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-5.3952, -4.9962,  0.2715,  ..., -0.4728,  1.5889, -1.9323],\n",
            "        [-4.3363, -3.8700, -1.3682,  ..., -0.3669,  1.6670, -1.8546],\n",
            "        [-6.3711, -6.0495, -1.1250,  ...,  2.0175,  0.7608, -1.0007],\n",
            "        ...,\n",
            "        [-6.7043, -6.2448,  0.0708,  ..., -0.9024, -0.4065, -1.4539],\n",
            "        [-6.7453, -6.5872, -1.5259,  ...,  0.8491,  0.2173, -0.3374],\n",
            "        [-4.8703, -4.5068, -0.6166,  ...,  0.0450,  1.0303, -0.8252]])\n",
            "\n",
            "tgt:\t tensor([14, 17, 18,  5,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 4056 / 7800\tLoss:\t2.679808\n",
            "pred:\t tensor([[-5.2103, -4.7459,  1.1497,  ..., -2.0099,  1.5484, -1.4153],\n",
            "        [-5.6637, -5.0667, -0.1064,  ..., -0.6479, -0.1924, -0.1257],\n",
            "        [-5.8537, -5.4306, -0.6814,  ..., -1.0370, -0.1360, -0.6890],\n",
            "        ...,\n",
            "        [-3.1406, -2.4826, -0.7716,  ..., -1.6694,  0.1375, -0.6449],\n",
            "        [-2.3507, -1.7532, -1.5545,  ..., -1.1192,  1.5009, -2.2680],\n",
            "        [-2.6171, -2.7656, -1.5909,  ...,  2.8132,  3.7965, -1.8338]])\n",
            "\n",
            "tgt:\t tensor([ 36,   6, 302,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4212 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.2734, -3.6216, -1.2183,  ...,  0.8242,  1.9635, -2.2203],\n",
            "        [-6.1580, -5.6036, -1.9698,  ...,  1.2394,  1.8700, -1.4206],\n",
            "        [-6.6275, -5.9549, -0.8631,  ..., -1.0816, -0.7308, -0.8346],\n",
            "        ...,\n",
            "        [-5.9653, -5.4501, -0.4339,  ..., -1.8716, -0.3638, -0.8821],\n",
            "        [-6.8673, -6.2706,  0.1292,  ..., -1.2586, -0.2008, -1.3260],\n",
            "        [-6.7208, -6.0687, -0.4416,  ..., -0.4221, -0.6480, -1.0804]])\n",
            "\n",
            "tgt:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4368 / 7800\tLoss:\t2.293785\n",
            "pred:\t tensor([[-5.8963, -5.4612, -0.8202,  ..., -1.0378, -0.5442, -0.6595],\n",
            "        [-7.5390, -7.0864, -0.8568,  ..., -0.1978, -1.1339, -0.7379],\n",
            "        [-1.9441, -1.2839, -0.9821,  ..., -1.6009,  1.2025, -1.7926],\n",
            "        ...,\n",
            "        [-5.2151, -4.6456, -1.6099,  ...,  0.3839,  3.3656, -3.2079],\n",
            "        [-7.3076, -6.7860, -0.9944,  ..., -1.0805,  0.7291, -2.0234],\n",
            "        [-2.6668, -2.3540, -1.0688,  ..., -0.0958,  2.1414, -1.6271]])\n",
            "\n",
            "tgt:\t tensor([ 14,  28, 122,   9, 113, 230,  11,   2,   0])\n",
            "\n",
            "iter 4524 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.4189, -3.8941, -1.2207,  ..., -1.9816, -0.9231, -0.8475],\n",
            "        [-3.0461, -3.0877,  0.6665,  ...,  1.1121,  2.3171,  0.9455],\n",
            "        [-4.8243, -4.3176, -0.3996,  ..., -1.4636, -0.6612, -0.8160],\n",
            "        ...,\n",
            "        [-4.8290, -4.2645, -1.9277,  ...,  1.7495,  0.2839, -0.3237],\n",
            "        [-4.6641, -3.7955, -2.0144,  ..., -0.7892,  1.3402, -2.2885],\n",
            "        [-5.0918, -4.8712, -1.9318,  ...,  3.4839,  2.2000, -1.4116]])\n",
            "\n",
            "tgt:\t tensor([ 15, 122, 121,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4680 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-5.5172, -4.9335, -1.2959,  ...,  0.3698,  0.6045, -1.3354],\n",
            "        [-6.0393, -5.3269, -1.1930,  ..., -0.3234,  3.3275, -3.9931],\n",
            "        [-3.3414, -2.8171, -0.4372,  ..., -2.5523, -0.0097, -1.3762],\n",
            "        ...,\n",
            "        [-5.9977, -5.8210,  1.4940,  ..., -1.0250, -0.2683, -0.3430],\n",
            "        [-7.1435, -6.7691,  0.4366,  ..., -1.2575, -0.5331, -1.5295],\n",
            "        [-5.5668, -5.0749,  1.2326,  ..., -2.1141,  1.2852, -1.4035]])\n",
            "\n",
            "tgt:\t tensor([38, 78,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 4836 / 7800\tLoss:\t2.107672\n",
            "pred:\t tensor([[-4.2379, -4.1948, -1.7191,  ...,  1.6624,  2.8365, -1.9519],\n",
            "        [-5.0264, -4.5592, -1.0175,  ..., -0.3462,  1.6388, -1.8740],\n",
            "        [-6.7589, -6.2462, -1.2118,  ...,  2.1497, -0.0258,  0.4300],\n",
            "        ...,\n",
            "        [-4.6063, -4.6945,  1.6552,  ...,  0.3796,  1.1007,  0.6389],\n",
            "        [-7.4429, -6.8236, -0.1520,  ..., -0.5612, -0.7666, -0.4508],\n",
            "        [-6.2906, -5.7073, -0.9971,  ..., -0.2023,  0.5479, -0.7631]])\n",
            "\n",
            "tgt:\t tensor([ 92, 277, 116,   3,  25,  72,   3,  11,   2])\n",
            "\n",
            "iter 4992 / 7800\tLoss:\t1.759200\n",
            "pred:\t tensor([[-4.8330, -4.7665, -0.5445,  ...,  0.1431,  2.6105, -3.3414],\n",
            "        [-6.7347, -6.1543, -2.2753,  ...,  1.1470,  0.4792,  0.0679],\n",
            "        [-6.3491, -5.8803, -0.5880,  ..., -1.3915, -0.1486, -0.8349],\n",
            "        ...,\n",
            "        [-3.7757, -3.0403, -2.7794,  ..., -0.2554,  1.1701, -1.0168],\n",
            "        [-6.0845, -5.6975, -1.5301,  ..., -0.7174, -0.3895, -0.0328],\n",
            "        [-4.9289, -4.4807, -2.7387,  ...,  3.4540,  2.5942, -1.6947]])\n",
            "\n",
            "tgt:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 5148 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-7.0050, -6.4825, -1.1952,  ...,  2.0174,  0.1228, -0.0967],\n",
            "        [-6.4813, -6.0853, -1.0408,  ..., -0.4199, -0.4396, -0.7478],\n",
            "        [-4.1241, -3.4246, -0.6429,  ..., -0.9504, -1.3598,  0.3734],\n",
            "        ...,\n",
            "        [-3.8083, -3.1945, -0.2621,  ..., -2.1305, -0.3126, -0.9332],\n",
            "        [-7.0448, -6.5582,  0.4516,  ..., -1.3531, -0.1152, -1.8475],\n",
            "        [-5.4946, -4.9267,  0.0255,  ..., -1.9927, -0.1833, -1.4178]])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 5304 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-5.7909, -5.5093, -0.6143,  ..., -1.6948,  0.0643, -1.3035],\n",
            "        [-6.8655, -6.4021, -0.7260,  ..., -0.7275, -0.3227, -0.7613],\n",
            "        [-4.7941, -4.1973,  1.4057,  ..., -0.5901,  1.6874, -0.4898],\n",
            "        ...,\n",
            "        [-6.7850, -6.2273, -0.5038,  ..., -0.9150, -0.6458, -0.5071],\n",
            "        [-6.4753, -5.9945, -0.4331,  ...,  1.2310,  3.2186, -1.2980],\n",
            "        [-3.8575, -3.3287, -2.6630,  ...,  3.0738,  2.2579, -1.9279]])\n",
            "\n",
            "tgt:\t tensor([15,  3, 75,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 5460 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-7.5131, -6.8966, -1.4703,  ..., -0.9375, -0.0959, -1.1457],\n",
            "        [-6.5629, -6.1024, -1.0307,  ..., -0.8609, -0.3883, -0.5385],\n",
            "        [-6.3107, -5.8813, -1.3889,  ..., -0.9884, -0.5200, -0.0939],\n",
            "        ...,\n",
            "        [-4.6462, -4.2698, -1.2933,  ...,  3.3394,  2.6935, -0.6587],\n",
            "        [-7.3247, -6.6530, -1.7899,  ...,  0.7858, -0.2950,  0.0181],\n",
            "        [-6.2376, -5.7388, -1.3964,  ..., -1.2030, -0.2980, -0.1185]])\n",
            "\n",
            "tgt:\t tensor([ 15, 240,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 5616 / 7800\tLoss:\t1.890437\n",
            "pred:\t tensor([[-5.8510, -5.1598, -2.2566,  ...,  0.4231,  1.4437, -2.4948],\n",
            "        [-6.7595, -5.8603, -2.6341,  ...,  1.1820,  1.9958, -3.1829],\n",
            "        [-6.1419, -5.6127, -1.6916,  ..., -1.0650,  1.3626, -2.6488],\n",
            "        ...,\n",
            "        [-6.8503, -6.3417, -2.3828,  ...,  1.7068,  0.1708,  0.1898],\n",
            "        [-7.4272, -6.9594, -1.3042,  ..., -0.1215, -0.3412, -0.5575],\n",
            "        [-5.9723, -5.6860, -1.4678,  ...,  2.3115,  0.5183, -1.6322]])\n",
            "\n",
            "tgt:\t tensor([38, 40,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 5772 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-2.5437, -2.7048, -0.8600,  ...,  0.4194,  3.7572, -2.6931],\n",
            "        [-7.2100, -6.6503, -0.7687,  ...,  1.5212, -0.2521, -0.6161],\n",
            "        [-5.8510, -5.4710, -1.4537,  ..., -0.5751,  3.6530, -4.0536],\n",
            "        ...,\n",
            "        [-5.3617, -4.7686, -0.3435,  ..., -1.0485,  0.2287, -1.0771],\n",
            "        [-4.4382, -4.4079, -0.3749,  ...,  0.2901,  2.3540, -3.0164],\n",
            "        [-6.6295, -6.1321, -1.0891,  ..., -1.1897, -0.3941, -0.3314]])\n",
            "\n",
            "tgt:\t tensor([ 92, 341, 221,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 5928 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-6.0772, -5.6258,  0.1768,  ..., -1.3198,  0.5793, -1.5466],\n",
            "        [-3.8639, -3.7084, -0.3916,  ..., -0.2067,  1.7323, -3.5032],\n",
            "        [-8.2508, -7.6391, -0.3953,  ...,  1.6333, -1.0155, -0.7113],\n",
            "        ...,\n",
            "        [-6.4910, -6.0188, -1.1606,  ..., -0.9221,  3.4206, -4.0655],\n",
            "        [-4.7715, -4.6837, -0.1684,  ...,  0.1604,  2.4400, -3.0245],\n",
            "        [-3.8275, -4.1646,  1.1205,  ..., -0.9982, -0.4058,  1.1028]])\n",
            "\n",
            "tgt:\t tensor([ 14,   3, 114,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6084 / 7800\tLoss:\t1.891413\n",
            "pred:\t tensor([[-6.9520, -6.3495, -1.0103,  ..., -0.3667, -1.5992,  0.5882],\n",
            "        [-5.4022, -5.1577, -0.8976,  ..., -0.3954, -0.1465,  0.5713],\n",
            "        [-5.5792, -4.9946, -2.4898,  ...,  0.7405,  0.6381, -0.4386],\n",
            "        ...,\n",
            "        [-8.1213, -7.4756,  0.2005,  ..., -1.1483, -1.3614, -1.3417],\n",
            "        [-5.2274, -4.6229, -1.0258,  ..., -0.7900,  0.2548, -1.1071],\n",
            "        [-6.8958, -6.8123, -1.6517,  ...,  3.6813,  0.7711, -0.0209]])\n",
            "\n",
            "tgt:\t tensor([14, 79, 29, 41,  3, 11,  2,  0,  0])\n",
            "\n",
            "iter 6240 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-6.0253, -5.2295, -1.6407,  ..., -0.2276,  4.1688, -4.1763],\n",
            "        [-1.8508, -1.4933, -3.0117,  ...,  2.5376,  3.0677, -1.7275],\n",
            "        [-5.2336, -4.6909, -0.7966,  ..., -1.7802,  0.4341, -0.5741],\n",
            "        ...,\n",
            "        [-5.3684, -4.8974, -2.9411,  ...,  3.9108,  2.0818, -1.4585],\n",
            "        [-6.4124, -5.8810, -0.8457,  ..., -0.7574, -0.5876,  0.0389],\n",
            "        [-5.2992, -4.7129, -1.5363,  ..., -0.1258,  0.1097, -0.1597]])\n",
            "\n",
            "tgt:\t tensor([36, 75,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 6396 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-7.1891, -6.6748,  0.1662,  ..., -1.9841, -0.2838, -1.8817],\n",
            "        [-6.3685, -5.8682,  0.9157,  ..., -1.5033, -1.2986, -0.7411],\n",
            "        [-5.3006, -4.8456, -2.1254,  ...,  1.8092,  1.6542, -1.6754],\n",
            "        ...,\n",
            "        [-6.3253, -5.9430, -0.8836,  ..., -0.8060,  1.6285, -1.8250],\n",
            "        [-5.4219, -4.9030, -1.6196,  ...,  2.0574,  0.7694, -1.7124],\n",
            "        [-7.9612, -7.1176, -1.8924,  ..., -1.6909, -0.3732, -2.8704]])\n",
            "\n",
            "tgt:\t tensor([14, 17,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 6552 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-6.8282, -6.1458, -0.4150,  ..., -1.1861, -0.8935,  0.2594],\n",
            "        [-6.2853, -5.9274, -1.6916,  ..., -0.8826, -0.2668,  0.2529],\n",
            "        [-5.4586, -5.2273, -2.5294,  ...,  4.5693,  2.2261, -1.1926],\n",
            "        ...,\n",
            "        [-4.2174, -4.1549, -1.5633,  ..., -0.1133,  2.2781, -2.6248],\n",
            "        [-6.0803, -5.7121, -2.0237,  ...,  4.2057,  1.3477, -0.1642],\n",
            "        [-6.2853, -5.9274, -1.6916,  ..., -0.8826, -0.2668,  0.2529]])\n",
            "\n",
            "tgt:\t tensor([ 14, 171, 120,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 6708 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-1.2428, -0.6230, -1.9894,  ..., -1.5072,  0.4017, -1.2598],\n",
            "        [-5.7600, -5.3896, -0.0730,  ..., -2.6571, -0.4530, -0.8374],\n",
            "        [-5.2782, -4.9841, -2.0989,  ...,  2.3309,  1.0255,  0.1681],\n",
            "        ...,\n",
            "        [-5.5894, -5.2408, -0.0233,  ..., -1.4844, -0.5335,  0.5941],\n",
            "        [-7.3683, -6.5987, -0.3064,  ..., -1.5138, -1.1597, -0.4572],\n",
            "        [-5.9812, -5.0620, -1.5953,  ..., -0.3836,  2.8822, -3.4376]])\n",
            "\n",
            "tgt:\t tensor([156, 254, 335,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6864 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-6.8876, -6.3097, -1.8521,  ...,  1.8352, -0.4176,  0.2780],\n",
            "        [-5.6360, -4.9509, -1.6726,  ..., -0.0096,  1.3653, -1.7971],\n",
            "        [-6.6340, -6.0592, -1.6179,  ...,  1.8356,  0.3309, -0.3580],\n",
            "        ...,\n",
            "        [-4.8533, -4.3881, -2.8961,  ...,  0.4236,  1.8090, -1.3082],\n",
            "        [-5.8299, -5.5023, -2.0899,  ..., -1.1758,  0.3067,  0.1138],\n",
            "        [-6.6635, -6.1848, -0.8090,  ..., -0.0783, -0.1487,  0.4908]])\n",
            "\n",
            "tgt:\t tensor([48,  3, 37,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 7020 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.7736, -4.0953, -0.9612,  ..., -1.6375, -1.1381,  0.2126],\n",
            "        [-7.5448, -6.7371, -1.9234,  ..., -1.7690,  0.4742, -1.4063],\n",
            "        [-6.0528, -5.5159, -0.7156,  ...,  3.5300,  1.8814, -0.9427],\n",
            "        ...,\n",
            "        [-3.9652, -3.3027, -3.0699,  ...,  3.0082,  1.9226, -1.6361],\n",
            "        [-5.6093, -5.0815, -1.0713,  ..., -1.9410,  0.2445, -0.3082],\n",
            "        [-7.2568, -6.8692, -1.5179,  ...,  2.7196,  1.1062, -1.0810]])\n",
            "\n",
            "tgt:\t tensor([ 14, 116, 286,  85,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 7176 / 7800\tLoss:\t1.793841\n",
            "pred:\t tensor([[-6.6151, -6.0299, -0.5540,  ..., -0.5985, -0.5512,  0.4557],\n",
            "        [-4.5691, -4.0106, -0.3378,  ..., -1.8038, -1.9653,  0.5022],\n",
            "        [-5.3741, -4.8430,  0.7486,  ..., -0.8021,  2.4613, -1.4836],\n",
            "        ...,\n",
            "        [-5.2594, -5.0671, -1.8691,  ...,  2.5134,  4.1853, -3.8710],\n",
            "        [-7.1294, -6.4463,  0.7016,  ..., -2.2017, -0.1783, -1.2429],\n",
            "        [-6.7924, -6.2851,  0.4298,  ..., -2.2575, -0.5667, -1.6829]])\n",
            "\n",
            "tgt:\t tensor([ 14,  33,  56, 147, 148,  11,   2,   0,   0])\n",
            "\n",
            "iter 7332 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-3.7955, -3.4426, -1.4687,  ...,  3.7700,  2.3180, -1.8966],\n",
            "        [-4.3522, -4.1987,  1.3863,  ..., -0.2157,  2.1828, -1.1567],\n",
            "        [-6.8522, -6.8235, -1.6877,  ...,  2.4324,  2.7842, -1.6640],\n",
            "        ...,\n",
            "        [-3.6922, -3.7364, -0.5655,  ...,  0.6151,  2.1019, -2.0560],\n",
            "        [-5.8665, -5.5865, -1.2345,  ..., -1.7300, -0.6304,  0.1774],\n",
            "        [-5.0428, -4.7709, -1.1977,  ..., -1.7813,  2.4104, -1.4584]])\n",
            "\n",
            "tgt:\t tensor([53, 75,  3,  3,  5,  2,  0,  0,  0])\n",
            "\n",
            "iter 7488 / 7800\tLoss:\t1.071651\n",
            "pred:\t tensor([[-7.0555, -6.5344, -0.9233,  ..., -1.8497, -1.1970, -0.1182],\n",
            "        [-6.7990, -6.2003, -0.6719,  ..., -0.8465, -1.7142,  0.0405],\n",
            "        [-6.4369, -6.0480, -1.2124,  ..., -0.8144, -0.0731, -0.3687],\n",
            "        ...,\n",
            "        [-5.5469, -4.8004,  0.6039,  ..., -1.0305, -0.3181, -0.7265],\n",
            "        [-5.4431, -5.0287, -1.2668,  ...,  3.4839,  2.3415, -0.5801],\n",
            "        [-6.9700, -6.5385, -1.0489,  ..., -1.5448, -0.0770, -0.2658]])\n",
            "\n",
            "tgt:\t tensor([ 14, 208, 377,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 7644 / 7800\tLoss:\tnan\n",
            "pred:\t tensor([[-4.9552, -4.5394, -0.1839,  ..., -2.2876,  0.5095, -2.2157],\n",
            "        [-5.8235, -5.1813, -2.2580,  ...,  3.5487,  0.3604,  0.1684],\n",
            "        [-5.7890, -5.4843,  1.1478,  ...,  1.5233,  1.5349,  0.0482],\n",
            "        ...,\n",
            "        [-7.3690, -6.8692, -2.0128,  ...,  2.3299, -0.7046,  0.1686],\n",
            "        [-4.2619, -4.1638, -0.9610,  ...,  0.6836,  2.0209, -2.8178],\n",
            "        [-6.1045, -6.2080, -1.6860,  ...,  1.1177,  4.9137, -3.3326]])\n",
            "\n",
            "tgt:\t tensor([ 52,  40, 123,  11,   2,   0,   0,   0,   0])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def train_lstm(net, train_iter, lr, epochs, device):\n",
        "  # training\n",
        "  if lr is None:\n",
        "    lr = 0.001\n",
        "  net = net.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      batch_size = train_data[0].shape[0]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      loss_list.append(loss.mean().detach().cpu())\n",
        "      optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
        "  return loss_list\n",
        "\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "lr = None\n",
        "##############################################################################\n",
        "# TODO: Find a good learning rate to train this model. Make sure your best\n",
        "# model is saved to the `lstm_net` variable.\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code (1 pts)\n",
        "lr = 1e-3\n",
        "# END OF YOUR CODE\n",
        "epochs = 50\n",
        "\n",
        "embedding_dim = 250\n",
        "hidden_size = 128\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "lstm_net = NMTLSTM(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size, device)\n",
        "\n",
        "lstm_loss_list = train_lstm(lstm_net, train_iter, lr, epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxuOta5JjFeu"
      },
      "source": [
        "### LSTM Loss Curve\n",
        "\n",
        "Plot the loss curve over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Juch6FWtjFeu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "f0022cf2-7c77-4d19-b80b-dc761dc030e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curve of LSTM Attention')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(np.arange(len(lstm_loss_list)), lstm_loss_list)\n",
        "plt.title('Loss Curve of LSTM Attention')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMn0v69mjFeu"
      },
      "source": [
        "Test the accuracy of your model. You should be able to get at least 68% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "I4-zQfjAjFeu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e4975e-05c9-4cb8-a470-c4908ca03e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "pred:\t ['entrez', '!', '.', '.', '.']\n",
            "\n",
            "tgt:\t ['unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', 'te', 'ferai', 'un', 'unk', '.', '.']\n",
            "\n",
            "tgt:\t ['je', \"t'en\", 'dois', 'une', '.', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['est-il', 'mort', '?', '?', '?', '?']\n",
            "\n",
            "tgt:\t ['est-il', 'unk', '?', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', 'suis', 'unk', '.', '.', '.']\n",
            "\n",
            "tgt:\t ['je', 'suis', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['unk', 'à', 'qui', 'que', 'ce', 'soit', '!', '!']\n",
            "\n",
            "tgt:\t ['demande', 'à', 'unk', 'qui', '!', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "Prediction Acc.: 0.7619\n"
          ]
        }
      ],
      "source": [
        "def comp_acc(pred, gt, valid_len):\n",
        "  N, T_gt = gt.shape[:2]\n",
        "  _, T_pr = pred.shape[:2]\n",
        "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
        "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
        "  len_mask = len_mask < valid_len[:, None]\n",
        "\n",
        "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
        "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
        "  return pred_acc\n",
        "\n",
        "def evaluate_lstm(net, train_iter, device):\n",
        "  acc_list = []\n",
        "  for i, train_data in enumerate(train_iter):\n",
        "    train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "    pred = net.predict(*train_data)\n",
        "\n",
        "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
        "    acc_list.append(pred_acc)\n",
        "    if i < 5:# print 5 samples from 5 batches\n",
        "      pred = pred[0].detach().cpu()\n",
        "      pred_seq = []\n",
        "      for t in range(MAX_LEN+1):\n",
        "        pred_wd = vocab_fra.index2word[pred[t].item()]\n",
        "        if pred_wd != 'eos':\n",
        "          pred_seq.append(pred_wd)\n",
        "\n",
        "      print('pred:\\t {}\\n'.format(pred_seq))\n",
        "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
        "\n",
        "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
        "\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "evaluate_lstm(lstm_net, train_iter, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "LOnx64VPjFeu"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}